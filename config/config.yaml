

model:
  num_layers: 6           # Number of encoder and decoder layers
  d_model: 512            # Dimension of the model
  num_heads: 8            # Number of attention heads
  d_ff: 2048              # Dimension of the feed-forward layer
  src_vocab_size: 10000   # Size of the source vocabulary
  tgt_vocab_size: 10000   # Size of the target vocabulary
  max_len: 512            # Maximum length of the input sequence
  dropout: 0.1            # Dropout probability

training:
  epochs: 10
  batch_size: 32
  learning_rate: 0.001
  save_model: true
  model_save_path: "./models/"

evaluation:
  batch_size: 32
  model_load_path: "./models/model_epoch_10.pt"  # Example path for the trained model

data:
  train_data_path: "./dataset/train_data.csv"
  eval_data_path: "./dataset/eval_data.csv"
  pad_token: 0
